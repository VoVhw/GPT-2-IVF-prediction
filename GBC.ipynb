{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxVNDBe+Tnhv9qJCwJiJfc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import KFold\n","import os\n","\n","# Load processed data from the pickle file\n","with open(\".../processed_data.pkl\", \"rb\") as f:\n","    data = pickle.load(f)\n","\n","structured_data = data[\"structured_data\"]\n","text_embeddings = data[\"text_embeddings\"]\n","target = data[\"target\"]\n","id_array = data[\"ids\"]\n","\n","# Define a function to run K-Fold cross-validation using GBC\n","def run_gbc_cv(features, labels, id_array, n_splits=5):\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    all_ids = []\n","    all_true_labels = []\n","    all_pred_probs = []\n","\n","    for train_idx, val_idx in kf.split(features):\n","        X_train = features[train_idx]\n","        X_val = features[val_idx]\n","        y_train = labels.iloc[train_idx].values\n","        y_val = labels.iloc[val_idx].values\n","\n","        # Initialize the Gradient Boosting Classifier with default parameters\n","        model = GradientBoostingClassifier(random_state=42)\n","        model.fit(X_train, y_train)\n","        pred_probs = model.predict_proba(X_val)[:, 1]\n","\n","        all_ids.extend(id_array[val_idx])\n","        all_true_labels.extend(y_val)\n","        all_pred_probs.extend(pred_probs)\n","\n","    results_df = pd.DataFrame({\n","        'ID': all_ids,\n","        'True_Label': all_true_labels,\n","        'Predicted_Probability': all_pred_probs\n","    })\n","    return results_df\n","\n","# Run CV for structured data only\n","results_structured = run_gbc_cv(structured_data, target, id_array, n_splits=5)\n","results_structured.to_csv(\"gbc_results_structured.csv\", index=False)\n","print(\"GBC CV results for structured data saved as 'gbc_results_structured.csv'.\")\n","\n","# Run CV for text embeddings only\n","results_text = run_gbc_cv(text_embeddings, target, id_array, n_splits=5)\n","results_text.to_csv(\"gbc_results_unstructured_gpt2.csv\", index=False)\n","print(\"GBC CV results for text embeddings saved as 'gbc_results_unstructured_gpt2.csv'.\")\n","\n","# Run CV for combined data (concatenation of structured and text embeddings)\n","combined_features = np.concatenate((structured_data, text_embeddings), axis=1)\n","results_combined = run_gbc_cv(combined_features, target, id_array, n_splits=5)\n","results_combined.to_csv(\"gbc_results_all_gpt2.csv\", index=False)\n","print(\"GBC CV results for combined data saved as 'gbc_results_all_gpt2.csv'.\")\n"],"metadata":{"id":"6gvbp8Of__IR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","\n","# File names (only filenames, no directory path)\n","file_structured = \"gbc_results_structured.csv\"\n","file_text = \"gbc_results_unstructured_gpt2.csv\"\n","file_combined = \"gbc_results_all_gpt2.csv\"\n","\n","# Load the CSV files\n","df_structured = pd.read_csv(file_structured)\n","df_text = pd.read_csv(file_text)\n","df_combined = pd.read_csv(file_combined)\n","\n","# Merge the three datasets on the \"ID\" column\n","merged_df = df_combined.merge(df_structured, on='ID', suffixes=('_combined', '_structured'))\\\n","                       .merge(df_text, on='ID', suffixes=('', '_text'))\n","\n","# Rename the true label column for consistency\n","merged_df['New_True_Label'] = merged_df['True_Label_combined']\n","\n","# Extract true labels and predicted probabilities\n","y_true = merged_df['New_True_Label']\n","y_pred_comb = merged_df['Predicted_Probability_combined']      # Combined results\n","y_pred_struct = merged_df['Predicted_Probability_structured']  # Structured-only results\n","y_pred_text = merged_df['Predicted_Probability']               # Unstructured-only results\n","\n","# Function to determine the optimal threshold (closest to the top-left corner of the ROC curve)\n","def find_optimal_cutoff(y_true, y_pred_prob):\n","    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n","    optimal_idx = np.argmin(np.sqrt((1 - tpr)**2 + (fpr)**2))\n","    optimal_threshold = thresholds[optimal_idx]\n","    return optimal_threshold, fpr, tpr, thresholds\n","\n","# Calculate the optimal threshold for each set of predictions\n","opt_thresh_comb, fpr_comb, tpr_comb, _ = find_optimal_cutoff(y_true, y_pred_comb)\n","opt_thresh_struct, fpr_struct, tpr_struct, _ = find_optimal_cutoff(y_true, y_pred_struct)\n","opt_thresh_text, fpr_text, tpr_text, _ = find_optimal_cutoff(y_true, y_pred_text)\n","\n","# Function to calculate evaluation metrics using a given threshold\n","def calculate_metrics(y_true, y_pred_prob, threshold):\n","    y_pred = [1 if prob >= threshold else 0 for prob in y_pred_prob]\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    sensitivity = recall_score(y_true, y_pred)  # Sensitivity equals recall\n","    specificity = tn / (tn + fp)\n","    return accuracy, precision, sensitivity, specificity\n","\n","# Calculate metrics for each modality\n","metrics_comb = calculate_metrics(y_true, y_pred_comb, opt_thresh_comb)\n","metrics_struct = calculate_metrics(y_true, y_pred_struct, opt_thresh_struct)\n","metrics_text = calculate_metrics(y_true, y_pred_text, opt_thresh_text)\n","\n","# Print out the optimal thresholds and metrics\n","print(f\"Optimal Threshold for Combined Data: {opt_thresh_comb}\")\n","print(f\"Metrics: Accuracy={metrics_comb[0]}, Precision={metrics_comb[1]}, Sensitivity={metrics_comb[2]}, Specificity={metrics_comb[3]}\")\n","print(f\"Optimal Threshold for Structured Data: {opt_thresh_struct}\")\n","print(f\"Metrics: Accuracy={metrics_struct[0]}, Precision={metrics_struct[1]}, Sensitivity={metrics_struct[2]}, Specificity={metrics_struct[3]}\")\n","print(f\"Optimal Threshold for Unstructured Data: {opt_thresh_text}\")\n","print(f\"Metrics: Accuracy={metrics_text[0]}, Precision={metrics_text[1]}, Sensitivity={metrics_text[2]}, Specificity={metrics_text[3]}\")\n","\n","# Save the merged CV results to CSV\n","merged_df.to_csv(\"merged_gbc_cv_results.csv\", index=False)\n","print(\"Merged GBC CV results saved as 'merged_gbc_cv_results.csv'.\")\n","\n","# Create a DataFrame to store the metrics results\n","results = {\n","    'Model': ['Combined Data', 'Structured Data', 'Unstructured Data'],\n","    'Optimal Cutoff': [opt_thresh_comb, opt_thresh_struct, opt_thresh_text],\n","    'Accuracy': [metrics_comb[0], metrics_struct[0], metrics_text[0]],\n","    'Precision': [metrics_comb[1], metrics_struct[1], metrics_text[1]],\n","    'Sensitivity': [metrics_comb[2], metrics_struct[2], metrics_text[2]],\n","    'Specificity': [metrics_comb[3], metrics_struct[3], metrics_text[3]]\n","}\n","results_df = pd.DataFrame(results)\n","print(results_df)\n","results_df.to_csv(\"gbc_cv_metrics_results.csv\", index=False)\n","print(\"GBC CV metrics results saved as 'gbc_cv_metrics_results.csv'.\")\n","\n","# Plot ROC curves for each modality\n","plt.figure()\n","plt.plot(fpr_comb, tpr_comb, color='red', lw=2, label=f'Combined (AUC = {auc(fpr_comb, tpr_comb):.3f})')\n","plt.plot(fpr_struct, tpr_struct, color='blue', lw=2, label=f'Structured (AUC = {auc(fpr_struct, tpr_struct):.3f})')\n","plt.plot(fpr_text, tpr_text, color='green', lw=2, label=f'Unstructured (AUC = {auc(fpr_text, tpr_text):.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curves for GBC Models')\n","plt.legend(loc=\"lower right\")\n","plt.savefig(\"gbc_ROC_three_models.png\", dpi=900)\n","plt.show()\n","\n","end_time = time.time()\n","print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n"],"metadata":{"id":"MxAk-zgf__qF"},"execution_count":null,"outputs":[]}]}